{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMapyIVljP0DoI9cm+WjjM1",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yfrund/ML_practice/blob/main/Tutorial_9_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Note: These are simplified examples. In real-world scenarios you'd rely on more data to make judgements (e.g., training and validation loss, test loss, etc.). These are just simple practice tasks."
   ],
   "metadata": {
    "id": "4x-mAC3HA1DR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1\n",
    "\n",
    "You're given a deep network with multiple hidden layers.\n",
    "\n",
    "We train it on some data.\n",
    "\n",
    "Observe gradient magnitude during backpropagation. Do you see any potential issues?\n"
   ],
   "metadata": {
    "id": "JgICHau9_8Kh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# we generate a synthetic dataset\n",
    "X = torch.randn(100, 10) # 100 samples, 10 features\n",
    "y = torch.randint(0, 2, (100,)).float()  # some binary targets\n",
    "\n",
    "# define a simple network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(10, 20)\n",
    "        self.hidden2 = nn.Linear(20, 20)\n",
    "        self.hidden3 = nn.Linear(20, 20)\n",
    "        self.hidden4 = nn.Linear(20, 20)\n",
    "        self.hidden5 = nn.Linear(20, 20)\n",
    "        self.output = nn.Linear(20, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden1(x))\n",
    "        x = self.activation(self.hidden2(x))\n",
    "        x = self.activation(self.hidden3(x))\n",
    "        x = self.activation(self.hidden4(x))\n",
    "        x = self.activation(self.hidden5(x))\n",
    "        return self.activation(self.output(x))\n",
    "\n",
    "# initializations\n",
    "model = SimpleNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# training\n",
    "for epoch in range(10):\n",
    "    print(f'\\n\\nEpoch #{epoch}.\\n\\n')\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs.squeeze(), y) #squeezing to ensure the same size between input and output\n",
    "    print(f'Loss: {loss.item()}.\\n')\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient magnitudes - norm computes the magnitude of vectors for example purposes\n",
    "    print(\"Gradient magnitudes for each layer:\\n\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name}: {param.grad.norm().item()}\\n\") # norm to compute vector magnitude\n",
    "\n",
    "    optimizer.step()\n"
   ],
   "metadata": {
    "id": "1dP3aiEZXHoQ",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "How can you solve the issue?"
   ],
   "metadata": {
    "id": "mKzLEVbxAPZE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Task 2.\n",
    "\n",
    "You're training another simple model.\n",
    "\n",
    "Observe the output. What's going on?"
   ],
   "metadata": {
    "id": "zX2dI8f0Z4mm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(1000, 10)  # 1000 samples, 10 features - a small dataset\n",
    "y = torch.randint(0, 2, (1000,)).float()  # binary targets\n",
    "\n",
    "# split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# convert to PyTorch tensors\n",
    "X_train, X_val, X_test = map(torch.tensor, (X_train, X_val, X_test))\n",
    "y_train, y_val, y_test = map(torch.tensor, (y_train, y_val, y_test))\n",
    "\n",
    "# define a simple network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(10, 100)\n",
    "        self.hidden2 = nn.Linear(100, 100) # lots of neurons\n",
    "        self.hidden3 = nn.Linear(100, 100)\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden1(x))\n",
    "        x = self.activation(self.hidden2(x))\n",
    "        x = self.activation(self.hidden3(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# initialisations\n",
    "model = SimpleNet()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# a simple function to calculate accuracies (this is a simple binary classificator - that's why we'll use accuracy as an example; perplexity would be more appropriate for language generation)\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    predictions = torch.sigmoid(outputs).round()\n",
    "    return (predictions == targets).float().mean().item()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train() # now the model is in training mode\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        outputs = model(batch_X).squeeze()\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train).squeeze()\n",
    "        val_outputs = model(X_val).squeeze()\n",
    "        test_outputs = model(X_test).squeeze()\n",
    "\n",
    "        train_acc = calculate_accuracy(train_outputs, y_train)\n",
    "        val_acc = calculate_accuracy(val_outputs, y_val)\n",
    "\n",
    "\n",
    "        print(f\"\\n\\nEpoch {epoch}:\")\n",
    "        print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"  Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test).squeeze()\n",
    "    test_acc = calculate_accuracy(test_outputs, y_test)\n",
    "\n",
    "# print final accuracies\n",
    "print(\"\\nFinal Accuracies:\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ],
   "metadata": {
    "id": "E2sJyQYmZ3vt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "How can you solve the issue?"
   ],
   "metadata": {
    "id": "e0LvBNwtdm0A"
   }
  }
 ]
}
